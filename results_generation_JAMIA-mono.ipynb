{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0336a9d8-e7ae-4b6e-a4b1-d91bcc74e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.offsetbox import VPacker, TextArea, DrawingArea, AnchoredOffsetbox, HPacker\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560f069d-9059-4fb3-a4ce-3a4fe9272d65",
   "metadata": {},
   "source": [
    "Load the log data from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db153ee8-91ef-440c-92bc-c2f60a533c66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "results_dir = '/Users/ispiero2/Documents/Research/Study 3 - Comparison of LLM for LRTI Symptom Extraction/Scripts/logsonly-11juli2025/'\n",
    "\n",
    "log_files = []\n",
    "\n",
    "# Loop over the folders with the log results\n",
    "for folder in os.listdir(results_dir):\n",
    "    folder_path = os.path.join(results_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        folds_dict = {}\n",
    "\n",
    "        # Loop over the folds within the folder\n",
    "        for fold_folder in os.listdir(folder_path):\n",
    "            fold_path = os.path.join(folder_path, fold_folder)\n",
    "\n",
    "            if os.path.isdir(fold_path) and fold_folder.startswith('fold_'):\n",
    "                log_path = os.path.join(fold_path, 'log_history.json')\n",
    "                with open(log_path) as f:\n",
    "                    log_file = json.load(f)\n",
    "\n",
    "                # Add the log results of the current fold\n",
    "                folds_dict[fold_folder] = log_file \n",
    "\n",
    "        # Add the log results of all folds to the respective modeling setting\n",
    "        if folds_dict:\n",
    "            log_files.append({folder: folds_dict})\n",
    "\n",
    "#log_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c743a-ea43-42c1-9220-ccd3e2cf9f26",
   "metadata": {},
   "source": [
    "Convert the log data into a raw DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6251c5d4-20dd-4173-806e-6a6f1f2fb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "rows = []\n",
    "\n",
    "for folder_entry in log_files:\n",
    "    for folder, folds in folder_entry.items():\n",
    "        for fold_name, log_data in folds.items():\n",
    "            \n",
    "            # Extract last evaluation log\n",
    "            eval_logs = log_data.get('Eval Logs', [])\n",
    "            if not eval_logs:\n",
    "                continue  \n",
    "            last_eval = eval_logs[-1]\n",
    "\n",
    "            # Split the folder key\n",
    "            parts = folder.split('_')\n",
    "            classifier = parts[0]\n",
    "\n",
    "            # Get name of the model which starts with either 'models', 'robbert', or 'medroberta'\n",
    "            model_start_idx = next((i for i, part in enumerate(parts) if part.startswith(('models', 'robbert', 'medroberta'))), None)\n",
    "            if model_start_idx is None:\n",
    "                continue \n",
    "            model = parts[model_start_idx]\n",
    "\n",
    "            # Get the name of the extracted symptom\n",
    "            symptom = '_'.join(parts[1:model_start_idx])\n",
    "\n",
    "            # Get the size of the training sample\n",
    "            sample_size = parts[model_start_idx + 1] if len(parts) > model_start_idx + 1 else ''\n",
    "\n",
    "            # Add row to the DataFrame with the respective results\n",
    "            rows.append({\n",
    "                'Classifier': classifier,\n",
    "                'Model': model,\n",
    "                'Symptom': symptom,\n",
    "                'Number of samples': sample_size,\n",
    "                'Fold': fold_name,\n",
    "                'Confusion matrix': last_eval.get('eval_confusion_matrix')\n",
    "            })\n",
    "\n",
    "# Create DataFrame\n",
    "df_raw = pd.DataFrame(rows)\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e379d80b-6dbf-40d2-a93e-aa025e2f7e9b",
   "metadata": {},
   "source": [
    "Clean the dataframe into the desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb86518-42bc-4a43-8fe0-674850dde93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# Clean the value names\n",
    "df_cleaned = df_raw.copy()\n",
    "\n",
    "df_cleaned['Classifier'] = df_cleaned['Classifier'].replace({'run': 'Direct', \n",
    "                                                             'pbrun': 'Prompt-based'})\n",
    "df_cleaned['Symptom'] = df_cleaned['Symptom'].replace({'Pijn_Borst': 'Chest pain', \n",
    "                                                       'Zieke_Indruk': 'Ill appearance',\n",
    "                                                       'Auscultatie': 'Crackles upon auscultation',\n",
    "                                                       'Hoesten': 'Cough',\n",
    "                                                       'Dyspnoe': 'Shortness of breath',\n",
    "                                                       'Rillingen': 'Chills',\n",
    "                                                       'Sputum': 'Sputum',\n",
    "                                                       'Verwardheid': 'Confusion',\n",
    "                                                       'Crepitaties': 'Crackles upon auscultation',\n",
    "                                                       'Koorts': 'Fever'})\n",
    "df_cleaned['Model'] = df_cleaned['Model'].replace({'models--pdelobelle--robbert-v2-dutch-base': 'RobBERT', \n",
    "                                                   'models--CLTL--MedRoBERTa.nl': 'MedRoBERTa.nl',\n",
    "                                                   'robbert-prompt': 'RobBERT',\n",
    "                                                   'medroberta-prompt': 'MedRoBERTa.nl'})\n",
    "df_cleaned['Number of samples'] = df_cleaned['Number of samples'].astype(str)\n",
    "df_cleaned['Number of samples'] = df_cleaned['Number of samples'].str.replace('-samples', '', regex=False)\n",
    "df_cleaned['Fold'] = df_cleaned['Fold'].str.replace('fold_', '', regex=False)\n",
    "\n",
    "# Convert number of samples to numeric\n",
    "df_cleaned['Number of samples'] = pd.to_numeric(df_cleaned['Number of samples'], errors='coerce')\n",
    "\n",
    "# Remove the sample size of 25 that is not used in the analysis (too low)\n",
    "df_cleaned = df_cleaned[df_cleaned['Number of samples'] != 25]\n",
    "\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f748d9-5ce9-4ad1-a1b2-17dd837d06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# Check the rows in which the confusion matrix is 2x2 (instead of 3x3)\n",
    "mask_2x2 = df_cleaned['Confusion matrix'].apply(\n",
    "    lambda x: isinstance(x, list) and len(x) == 2 and all(isinstance(row, list) and len(row) == 2 for row in x)\n",
    ")\n",
    "df_2x2 = df_cleaned[mask_2x2]\n",
    "\n",
    "# Save the row indices\n",
    "indices_2x2 = df_cleaned[mask_2x2].index\n",
    "\n",
    "df_2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e3fd99-c988-4d76-9c22-28ca1c3191ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# Create a dataframe with the counts for class within each symptom across the folds:\n",
    "columns = ['Fold', 'Class', 'Fever', 'Cough', 'Shortness of breath', 'Sputum', 'Confusion', 'Chest pain',\n",
    "          'Chills', 'Ill appearence', 'Crackles upon auscultation']\n",
    "data = [\n",
    "    [0, 0, 147, 6, 58, 14, 9, 15, 2, 81, 259],\n",
    "    [0, 1, 98, 309, 165, 94, 6, 48, 20, 46, 76],\n",
    "    [0, 2, 147, 77, 169, 284, 377, 329, 370, 256, 57],\n",
    "    [1, 0, 127,4,37,5,4,16,1,75,222],\n",
    "    [1, 1, 94,299,159,87,8,37,19,42,107],\n",
    "    [1, 2, 171,89,196,300,380,339,372,275,63],\n",
    "    [2, 0, 113,7,41,11,7,12,3,90,257],\n",
    "    [2, 1, 112,289,150,87,1,51,18,38,89],\n",
    "    [2, 2, 166,95,200,293,383,328,370,263,45],\n",
    "    [3, 0, 139,4,56,13,7,22,1,83,238],\n",
    "    [3, 1, 91,299,152,105,5,37,17,41,90],\n",
    "    [3, 2, 161,88,183,273,379,332,373,267,63],\n",
    "    [4, 0, 125,6,46,12,10,12,0,57,232],\n",
    "    [4, 1, 99,294,147,91,2,37,16,44,97],\n",
    "    [4, 2, 167,91,198,288,379,342,375,290,62]\n",
    "]\n",
    "df_counts = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5905b37c-d3ec-44ab-9a98-6da942d3e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# Expand the 2x2 matrices to 3x3 by adding zeros in case a class happened to not occur in the data sample\n",
    "\n",
    "def expand_confusion_matrix(cm, true_counts, num_classes=3):\n",
    "    \n",
    "    present_classes = [cls for cls, count in enumerate(true_counts) if count > 0]\n",
    "    full_cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    # Map present classes to cm indices\n",
    "    row_map = {i: cls for i, cls in enumerate(present_classes)}\n",
    "    col_map = row_map  \n",
    "\n",
    "    for i, row_class in row_map.items():\n",
    "        for j, col_class in col_map.items():\n",
    "            full_cm[row_class, col_class] = cm[i, j]\n",
    "    \n",
    "    return full_cm\n",
    "\n",
    "\n",
    "def fix_confusion_matrices(df, df_counts):\n",
    "\n",
    "    df['Fold'] = df['Fold'].astype(int)\n",
    "    df_counts['Fold'] = df_counts['Fold'].astype(int)\n",
    "    \n",
    "    fixed_matrices = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        cm = np.array(row['Confusion matrix'])\n",
    "        symptom = row['Symptom']\n",
    "        fold = row['Fold']\n",
    "        \n",
    "        if cm.shape == (3, 3):\n",
    "            fixed_matrices.append(cm)\n",
    "            continue  \n",
    "\n",
    "        # Get true class counts from df_counts\n",
    "        counts_row = df_counts[df_counts['Fold'] == fold]\n",
    "        if counts_row.empty:\n",
    "            raise ValueError(f\"No matching fold={fold} found in df_counts.\")\n",
    "        \n",
    "        counts = counts_row[[symptom]].reset_index(drop=True)\n",
    "        if counts.shape[0] != 3:\n",
    "            raise ValueError(f\"Expected 3 class rows for fold={fold} in df_counts.\")\n",
    "\n",
    "        true_counts = counts[symptom].tolist()\n",
    "        expanded_cm = expand_confusion_matrix(cm, true_counts)\n",
    "        fixed_matrices.append(expanded_cm)\n",
    "\n",
    "    df['Confusion matrix'] = fixed_matrices\n",
    "    return df\n",
    "\n",
    "df_correct = fix_confusion_matrices(df_cleaned, df_counts)\n",
    "df_correct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d78294b-5525-4fbe-a63d-addd2af278d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# Check how many 2x2 matrices there are in the DataFrame (should be zero)\n",
    "len(df_correct[df_correct['Confusion matrix'].apply(\n",
    "    lambda x: isinstance(x, list) and len(x) == 2 and all(isinstance(row, list) and len(row) == 2 for row in x)\n",
    ")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da6794-41cc-41aa-b84e-1d0e40d9402b",
   "metadata": {},
   "source": [
    "Compute the (micro/macro/per-class) averages of recall, precision, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ebe5415-1770-4c83-b3fb-fdd40970b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "def compute_metrics(conf_matrix):\n",
    "    \n",
    "    conf_matrix = np.array(conf_matrix)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    num_classes = 3\n",
    "\n",
    "    for i in range(num_classes):      \n",
    "        for j in range(num_classes):   \n",
    "            y_true += [i] * conf_matrix[i, j]\n",
    "            y_pred += [j] * conf_matrix[i, j]\n",
    "\n",
    "    labels = [0, 1, 2]\n",
    "\n",
    "    # Global metrics\n",
    "    micro_precision = precision_score(y_true, y_pred, average='micro', labels=labels, zero_division=0)\n",
    "    micro_recall = recall_score(y_true, y_pred, average='micro', labels=labels, zero_division=0)\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro', labels=labels, zero_division=0)\n",
    "\n",
    "    macro_precision = precision_score(y_true, y_pred, average='macro', labels=labels, zero_division=0)\n",
    "    macro_recall = recall_score(y_true, y_pred, average='macro', labels=labels, zero_division=0)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', labels=labels, zero_division=0)\n",
    "\n",
    "    # Per-class metrics\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, labels=labels,\n",
    "        output_dict=True, zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'Micro-precision': micro_precision,\n",
    "        'Micro-recall': micro_recall,\n",
    "        'Micro-F1': micro_f1,\n",
    "        'Macro-precision': macro_precision,\n",
    "        'Macro-recall': macro_recall,\n",
    "        'Macro-F1': macro_f1,\n",
    "        \"Precision 'present'\": report['1']['precision'],\n",
    "        \"Recall 'present'\": report['1']['recall'],\n",
    "        \"F1 'present'\": report['1']['f1-score'],\n",
    "        \"Precision 'absent'\": report['0']['precision'],\n",
    "        \"Recall 'absent'\": report['0']['recall'],\n",
    "        \"F1 'absent'\": report['0']['f1-score'],\n",
    "        \"Precision 'not reported'\": report['2']['precision'],\n",
    "        \"Recall 'not reported'\": report['2']['recall'],\n",
    "        \"F1 'not reported'\": report['2']['f1-score'],\n",
    "    }\n",
    "\n",
    "metrics_df = df_correct['Confusion matrix'].apply(compute_metrics).apply(pd.Series)\n",
    "df_final = pd.concat([df_correct, metrics_df], axis=1)\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45afb376-d0ae-46cd-b15e-4e1b06fa842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# Compute the averages of the metrics across folds\n",
    "averaged_df = df_final.groupby(['Classifier', 'Model',  'Symptom', 'Number of samples'])[['Micro-precision',\n",
    "                                                                                          'Micro-recall',\n",
    "                                                                                          'Micro-F1',\n",
    "                                                                                          'Macro-precision',\n",
    "                                                                                          'Macro-recall',\n",
    "                                                                                          'Macro-F1',\n",
    "                                                                                          \"Precision 'present'\",\n",
    "                                                                                          \"Recall 'present'\",\n",
    "                                                                                          \"F1 'present'\",\n",
    "                                                                                          \"Precision 'absent'\",\n",
    "                                                                                          \"Recall 'absent'\",\n",
    "                                                                                          \"F1 'absent'\",\n",
    "                                                                                          \"Precision 'not reported'\",\n",
    "                                                                                          \"Recall 'not reported'\",\n",
    "                                                                                          \"F1 'not reported'\"]].mean().reset_index()\n",
    "\n",
    "averaged_df['Number of samples'] = averaged_df['Number of samples'].astype(str)\n",
    "averaged_df_styled = averaged_df.style.background_gradient(\n",
    "    cmap=LinearSegmentedColormap.from_list(\n",
    "    'soft_rgy', ['#ffcccc', '#fff2b2', '#ccffcc']\n",
    "),  \n",
    "    axis=None       \n",
    ")\n",
    "\n",
    "averaged_df_styled.to_excel(\"Table_all.xlsx\", engine='openpyxl')\n",
    "\n",
    "averaged_df_styled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4793e0f-263d-4861-b93b-822742bf106b",
   "metadata": {},
   "source": [
    "Create a subset of the largest number of training data used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c26f2be-cfbd-4b90-be09-5a7e4fb248a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "averaged_df_copy = averaged_df.copy()\n",
    "averaged_df_copy['Number of samples'] = averaged_df_copy['Number of samples'].astype(str)\n",
    "averaged_df_copy = averaged_df_copy[averaged_df_copy['Number of samples'] == '1600'].style.background_gradient(\n",
    "    cmap=LinearSegmentedColormap.from_list(\n",
    "    'soft_rgy', ['#ffcccc', '#fff2b2', '#ccffcc']\n",
    "),  \n",
    "    axis=None       \n",
    ")\n",
    "averaged_df_copy.to_excel(\"Table_1600.xlsx\", engine='openpyxl')\n",
    "averaged_df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c886e7-1f27-4d68-a8d7-922889d6f3f0",
   "metadata": {},
   "source": [
    "Derive the min, max and mean values of the metrics for in the results section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d7bdc02-1e99-4793-b03d-25293e1b11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# Select only the results for the largest number of samples\n",
    "averaged_df['Number of samples'] = pd.to_numeric(averaged_df['Number of samples'], errors='coerce')\n",
    "filtered_df = averaged_df[averaged_df['Number of samples'].isin([1600, 3])]\n",
    "#filtered_df = averaged_df\n",
    "grouped_results = filtered_df.groupby(['Classifier', 'Model'])[['Micro-precision',\n",
    "            'Micro-recall',\n",
    "            'Micro-F1',\n",
    "            'Macro-precision',\n",
    "            'Macro-recall',\n",
    "            'Macro-F1',\n",
    "            \"Precision 'present'\",\n",
    "            \"Recall 'present'\",\n",
    "            \"F1 'present'\",\n",
    "            \"Precision 'absent'\",\n",
    "            \"Recall 'absent'\",\n",
    "            \"F1 'absent'\",\n",
    "            \"Precision 'not reported'\",\n",
    "            \"Recall 'not reported'\",\n",
    "            \"F1 'not reported'\"]].agg(['min', 'max', 'mean','median']).reset_index()\n",
    "\n",
    "grouped_results.round(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf48d8-1801-4b22-9966-cf7df7226f89",
   "metadata": {},
   "source": [
    "Create a figure for the results of direct classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8206fdff-195d-43dc-bef2-1ea5290a0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "averaged_df['Number of samples'] = pd.to_numeric(averaged_df['Number of samples'], errors='coerce')\n",
    "\n",
    "# Select rows where Classifier is 'Direct'\n",
    "direct_classifier_df = averaged_df[averaged_df['Classifier'] == 'Direct']\n",
    "\n",
    "# Convert wide to long format\n",
    "df_long = direct_classifier_df.melt(\n",
    "    id_vars=['Model', 'Symptom', 'Number of samples'],\n",
    "    value_vars=['Micro-precision',\n",
    "            'Micro-recall',\n",
    "            'Micro-F1',\n",
    "            'Macro-precision',\n",
    "            'Macro-recall',\n",
    "            'Macro-F1',\n",
    "            \"Precision 'present'\",\n",
    "            \"Recall 'present'\",\n",
    "            \"F1 'present'\",\n",
    "            \"Precision 'absent'\",\n",
    "            \"Recall 'absent'\",\n",
    "            \"F1 'absent'\",\n",
    "            \"Precision 'not reported'\",\n",
    "            \"Recall 'not reported'\",\n",
    "            \"F1 'not reported'\"],\n",
    "    var_name='metric',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "# Create the mirrored value column to plot the models in a vertical mirrored barplot\n",
    "df_long['mirrored_value'] = df_long.apply(\n",
    "    lambda row: row['value'] if row['Model'] == 'RobBERT' else -row['value'],\n",
    "    axis=1\n",
    ")\n",
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd74eb5-1e91-43c1-b6d1-114e02148c43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# Color palette for MedRoBERTa.nl (blue) and RobBERT (orange)\n",
    "subcategory_colors_medroberta = {\n",
    "    'Recall': {\n",
    "        'Macro-recall': '#4D4D4D',         # dark grey (cool)\n",
    "        \"Recall 'present'\": '#7F7F7F',     # medium grey (cool)\n",
    "        \"Recall 'absent'\": '#A6A6A6',      # light-medium grey (cool)\n",
    "        \"Recall 'not reported'\": '#D9D9D9' # light grey (cool)\n",
    "    },\n",
    "    'Precision': {\n",
    "        'Macro-precision': '#4D4D4D',\n",
    "        \"Precision 'present'\": '#7F7F7F',\n",
    "        \"Precision 'absent'\": '#A6A6A6',\n",
    "        \"Precision 'not reported'\": '#D9D9D9'\n",
    "    },\n",
    "    'F1': {\n",
    "        'Macro-F1': '#4D4D4D',\n",
    "        \"F1 'present'\": '#7F7F7F',\n",
    "        \"F1 'absent'\": '#A6A6A6',\n",
    "        \"F1 'not reported'\": '#D9D9D9'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Color palette for RobBERT (warm greys)\n",
    "subcategory_colors_roberta = {\n",
    "    'Recall': {\n",
    "        'Macro-recall': '#5A5047',         # dark grey (warm)\n",
    "        \"Recall 'present'\": '#85786E',     # medium grey (warm)\n",
    "        \"Recall 'absent'\": '#B0A49C',      # light-medium grey (warm)\n",
    "        \"Recall 'not reported'\": '#D8D2CD' # light grey (warm)\n",
    "    },\n",
    "    'Precision': {\n",
    "        'Macro-precision': '#5A5047',\n",
    "        \"Precision 'present'\": '#85786E',\n",
    "        \"Precision 'absent'\": '#B0A49C',\n",
    "        \"Precision 'not reported'\": '#D8D2CD'\n",
    "    },\n",
    "    'F1': {\n",
    "        'Macro-F1': '#5A5047',\n",
    "        \"F1 'present'\": '#85786E',\n",
    "        \"F1 'absent'\": '#B0A49C',\n",
    "        \"F1 'not reported'\": '#D8D2CD'\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics = ['Recall', 'Precision', 'F1']\n",
    "subcategories_per_metric = {\n",
    "    'Recall': [\"Recall 'not reported'\", \"Recall 'absent'\", \"Recall 'present'\", 'Macro-recall'],\n",
    "    'Precision': [\"Precision 'not reported'\", \"Precision 'absent'\", \"Precision 'present'\", 'Macro-precision'],\n",
    "    'F1': [\"F1 'not reported'\", \"F1 'absent'\", \"F1 'present'\", 'Macro-F1']\n",
    "}\n",
    "\n",
    "# Grid setup: each column corresponds to a metric (Recall, Precision, F1) and each row to a symptom\n",
    "num_symptoms = len(df_long['Symptom'].unique())\n",
    "fig, axes = plt.subplots(3, num_symptoms, figsize=(40, 20), sharex=True, sharey=True)\n",
    "\n",
    "bar_width = 0.15\n",
    "ytick_spacing = 1\n",
    "font_params = {'fontsize': 18}  \n",
    "\n",
    "# Loop over metrics (columns)\n",
    "for col_idx, metric in enumerate(metrics):\n",
    "    subcats = subcategories_per_metric[metric]\n",
    "\n",
    "    # Loop over symptoms (rows)\n",
    "    for row_idx, symptom in enumerate(df_long['Symptom'].unique()):\n",
    "        ax = axes[col_idx, row_idx] if axes.ndim == 2 else axes[row_idx]\n",
    "        symptom_data = df_long[df_long['Symptom'] == symptom]\n",
    "        ytick_count = len(symptom_data['Number of samples'].unique())\n",
    "        y_positions = np.arange(ytick_count) * ytick_spacing\n",
    "\n",
    "        # Loop over subcategories\n",
    "        for k, subcat in enumerate(subcats):\n",
    "            for model in ['MedRoBERTa.nl', 'RobBERT']:\n",
    "                subset = symptom_data[\n",
    "                    (symptom_data['metric'] == subcat) &\n",
    "                    (symptom_data['Model'] == model)\n",
    "                ]\n",
    "                training_sizes = sorted(subset['Number of samples'].unique())\n",
    "                values = [\n",
    "                    subset[subset['Number of samples'] == ts]['mirrored_value'].values[0]\n",
    "                    if not subset[subset['Number of samples'] == ts].empty else np.nan\n",
    "                    for ts in training_sizes\n",
    "                ]\n",
    "                y_pos = y_positions + k * bar_width - bar_width  \n",
    "\n",
    "                # Apply subcategory-specific color\n",
    "                if model == 'MedRoBERTa.nl':\n",
    "                    color = subcategory_colors_medroberta[metric].get(subcat, '#cccccc')\n",
    "                else:\n",
    "                    color = subcategory_colors_roberta[metric].get(subcat, '#cccccc')\n",
    "\n",
    "                ax.barh(\n",
    "                    y=y_pos,\n",
    "                    width=values,\n",
    "                    height=bar_width,\n",
    "                    color=color,\n",
    "                    edgecolor='none'\n",
    "                )\n",
    "\n",
    "        # Y-axis labels for the first row only\n",
    "        if row_idx == 0:\n",
    "            ax.set_yticks(y_positions + 0.15)\n",
    "            ax.set_yticklabels([200, 400, 600, 800, 1000, 1200, 1400, 1600], fontsize=font_params['fontsize'])\n",
    "            ax.set_ylabel(\"Number of samples\", fontsize=font_params['fontsize'])\n",
    "        else:\n",
    "            ax.set_yticklabels([200, 400, 600, 800, 1000, 1200, 1400, 1600])\n",
    "\n",
    "        ax.set_xlim(-1, 1)\n",
    "        ax.axvline(0, color='black', linewidth=0.5)\n",
    "        ax.grid(True, axis='x', linestyle='--', linewidth=0.5, color='black')\n",
    "\n",
    "        # Remove x-tick labels for non-last rows\n",
    "        if row_idx != num_symptoms - 1:\n",
    "            ax.set_xticklabels([])\n",
    "\n",
    "        # Set custom x-tick labels\n",
    "        xticks = np.linspace(-1, 1, 9)\n",
    "        xticklabels = ['1', '0.75', '0.5', '0.25', '0', '0.25', '0.50', '0.75', '1']\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xticklabels, fontsize=font_params['fontsize']-5)\n",
    "\n",
    "        ax.text(0.442, 1.005, 'MedRoBERTa.nl vs RobBERT', transform=ax.transAxes,\n",
    "                fontsize=font_params['fontsize']-1, ha='center', va='bottom')\n",
    "\n",
    "# Add \"Value\" label for each column in the last row\n",
    "for col_idx in range(num_symptoms):\n",
    "    ax = axes[len(metrics) - 1, col_idx]  \n",
    "    ax.set_xlabel(\"Value\", fontsize=font_params['fontsize'])  \n",
    "\n",
    "# Build and add legend\n",
    "# Define unified submetric order and labels\n",
    "legend_submetrics = ['Macro', \"'present'\", \"'absent'\", \"'not reported'\"]\n",
    "label_mapping = {\n",
    "    'Macro': 'Macro',\n",
    "    \"'present'\": \"Present\",\n",
    "    \"'absent'\": \"Absent\",\n",
    "    \"'not reported'\": \"Not reported\"\n",
    "}\n",
    "\n",
    "# Use Recall mappings as representative for colors (they are consistent across metrics)\n",
    "color_keys_med = ['Macro-recall', \"Recall 'present'\", \"Recall 'absent'\", \"Recall 'not reported'\"]\n",
    "color_keys_rob = ['Macro-recall', \"Recall 'present'\", \"Recall 'absent'\", \"Recall 'not reported'\"]\n",
    "\n",
    "legend_items_medroberta = [TextArea(\"MedRoBERTa.nl\", textprops=dict(fontsize=font_params['fontsize'], ha='center'))]\n",
    "legend_items_roberta = [TextArea(\"RobBERT\", textprops=dict(fontsize=font_params['fontsize'], ha='center'))]\n",
    "\n",
    "# Add submetric entries just once\n",
    "for submetric_label, color_key_med, color_key_rob in zip(legend_submetrics, color_keys_med, color_keys_rob):\n",
    "    # MedRoBERTa.nl\n",
    "    patch_blue = DrawingArea(20, 10, 0, 0)\n",
    "    rect_blue = Rectangle((0, 0), 20, 10, fc=subcategory_colors_medroberta['Recall'][color_key_med], edgecolor='none')\n",
    "    patch_blue.add_artist(rect_blue)\n",
    "    label_blue = TextArea(label_mapping[submetric_label], textprops=dict(fontsize=font_params['fontsize'], ha='left'))\n",
    "    legend_items_medroberta.append(HPacker(children=[patch_blue, label_blue], align=\"left\", pad=0, sep=6))\n",
    "\n",
    "    # RobBERT\n",
    "    patch_orange = DrawingArea(20, 10, 0, 0)\n",
    "    rect_orange = Rectangle((0, 0), 20, 10, fc=subcategory_colors_roberta['Recall'][color_key_rob], edgecolor='none')\n",
    "    patch_orange.add_artist(rect_orange)\n",
    "    label_orange = TextArea(label_mapping[submetric_label], textprops=dict(fontsize=font_params['fontsize'], ha='left'))\n",
    "    legend_items_roberta.append(HPacker(children=[patch_orange, label_orange], align=\"left\", pad=0, sep=6))\n",
    "\n",
    "# Combine and place the legend\n",
    "final_legend = VPacker(\n",
    "    children=legend_items_medroberta + [TextArea(\"\", textprops=dict(fontsize=font_params['fontsize']))] + legend_items_roberta,\n",
    "    align=\"left\", pad=0, sep=18\n",
    ")\n",
    "\n",
    "anchored_box = AnchoredOffsetbox(\n",
    "    loc='center left',\n",
    "    child=final_legend,\n",
    "    pad=0.,\n",
    "    frameon=False,\n",
    "    bbox_to_anchor=(0.94, 0.5),\n",
    "    bbox_transform=fig.transFigure,\n",
    "    borderpad=0.\n",
    ")\n",
    "fig.add_artist(anchored_box)\n",
    "\n",
    "\n",
    "# Subtitle\n",
    "# Add column headers\n",
    "for idx, metric in enumerate(df_long['Symptom'].unique()):\n",
    "    fig.text(\n",
    "        x=(0.065 + idx * 0.102),  \n",
    "        y=0.98,\n",
    "        s=metric,\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=font_params['fontsize'] + 2, \n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Add row labels (Recall, Precision, F1) from top to bottom\n",
    "row_labels = ['Recall', 'Precision', 'F1-score']\n",
    "for idx, label in enumerate(row_labels):\n",
    "    fig.text(\n",
    "        x=-0.01,  \n",
    "        y=0.81 - idx * 0.31, \n",
    "        s=label,\n",
    "        ha='left',\n",
    "        va='center',\n",
    "        rotation='vertical',\n",
    "        fontsize=font_params['fontsize'] + 2,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "fig.suptitle(\"Direct classification\", fontsize=20, fontweight='bold', y=1.02, x=0.01, ha='left')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.93, 1])\n",
    "plt.savefig('Direct_classifiers_mono.tiff', format='tiff',\n",
    "            bbox_inches='tight', \n",
    "            pad_inches=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d36905-6842-485d-9077-17a434544e75",
   "metadata": {},
   "source": [
    "Create a figure for the results of prompt-based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7c96b93-94df-4f4c-bfea-0e481e7eb61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# Select rows where Classifier is 'Prompt-based'\n",
    "prompt_based_df = averaged_df[averaged_df['Classifier'] == 'Prompt-based']\n",
    "\n",
    "# Convert wide to long format\n",
    "df_long = prompt_based_df.melt(\n",
    "    id_vars=['Model', 'Symptom', 'Number of samples'],\n",
    "    value_vars=['Micro-precision',\n",
    "            'Micro-recall',\n",
    "            'Micro-F1',\n",
    "            'Macro-precision',\n",
    "            'Macro-recall',\n",
    "            'Macro-F1',\n",
    "            \"Precision 'present'\",\n",
    "            \"Recall 'present'\",\n",
    "            \"F1 'present'\",\n",
    "            \"Precision 'absent'\",\n",
    "            \"Recall 'absent'\",\n",
    "            \"F1 'absent'\",\n",
    "            \"Precision 'not reported'\",\n",
    "            \"Recall 'not reported'\",\n",
    "            \"F1 'not reported'\"],\n",
    "    var_name='metric',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "# Create the mirrored value column to plot the models in a vertical mirrored barplot\n",
    "df_long['mirrored_value'] = df_long.apply(\n",
    "    lambda row: row['value'] if row['Model'] == 'RobBERT' else -row['value'],\n",
    "    axis=1\n",
    ")\n",
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7797bd9-aefc-431d-a046-f7bfb0411ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# Color palette for MedRoBERTa.nl (blue) and RobBERT (orange)\n",
    "subcategory_colors_medroberta = {\n",
    "    'Recall': {\n",
    "        'Macro-recall': '#4D4D4D',         # dark grey (cool)\n",
    "        \"Recall 'present'\": '#7F7F7F',     # medium grey (cool)\n",
    "        \"Recall 'absent'\": '#A6A6A6',      # light-medium grey (cool)\n",
    "        \"Recall 'not reported'\": '#D9D9D9' # light grey (cool)\n",
    "    },\n",
    "    'Precision': {\n",
    "        'Macro-precision': '#4D4D4D',\n",
    "        \"Precision 'present'\": '#7F7F7F',\n",
    "        \"Precision 'absent'\": '#A6A6A6',\n",
    "        \"Precision 'not reported'\": '#D9D9D9'\n",
    "    },\n",
    "    'F1': {\n",
    "        'Macro-F1': '#4D4D4D',\n",
    "        \"F1 'present'\": '#7F7F7F',\n",
    "        \"F1 'absent'\": '#A6A6A6',\n",
    "        \"F1 'not reported'\": '#D9D9D9'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Color palette for RobBERT (warm greys)\n",
    "subcategory_colors_roberta = {\n",
    "    'Recall': {\n",
    "        'Macro-recall': '#5A5047',         # dark grey (warm)\n",
    "        \"Recall 'present'\": '#85786E',     # medium grey (warm)\n",
    "        \"Recall 'absent'\": '#B0A49C',      # light-medium grey (warm)\n",
    "        \"Recall 'not reported'\": '#D8D2CD' # light grey (warm)\n",
    "    },\n",
    "    'Precision': {\n",
    "        'Macro-precision': '#5A5047',\n",
    "        \"Precision 'present'\": '#85786E',\n",
    "        \"Precision 'absent'\": '#B0A49C',\n",
    "        \"Precision 'not reported'\": '#D8D2CD'\n",
    "    },\n",
    "    'F1': {\n",
    "        'Macro-F1': '#5A5047',\n",
    "        \"F1 'present'\": '#85786E',\n",
    "        \"F1 'absent'\": '#B0A49C',\n",
    "        \"F1 'not reported'\": '#D8D2CD'\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics = ['Recall', 'Precision', 'F1']\n",
    "subcategories_per_metric = {\n",
    "    'Recall': [\"Recall 'not reported'\", \"Recall 'absent'\", \"Recall 'present'\", 'Macro-recall'],\n",
    "    'Precision': [\"Precision 'not reported'\", \"Precision 'absent'\", \"Precision 'present'\", 'Macro-precision'],\n",
    "    'F1': [\"F1 'not reported'\", \"F1 'absent'\", \"F1 'present'\", 'Macro-F1']\n",
    "}\n",
    "\n",
    "# Grid setup: each column corresponds to a metric (Recall, Precision, F1) and each row to a symptom\n",
    "num_symptoms = len(df_long['Symptom'].unique())\n",
    "fig, axes = plt.subplots(3, num_symptoms, figsize=(40, 20), sharex=True, sharey=True)  \n",
    "\n",
    "bar_width = 0.15\n",
    "ytick_spacing = 1\n",
    "font_params = {'fontsize': 18}  \n",
    "\n",
    "# Loop over metrics (columns)\n",
    "for col_idx, metric in enumerate(metrics):\n",
    "    subcats = subcategories_per_metric[metric]\n",
    "\n",
    "    # Loop over symptoms (rows)\n",
    "    for row_idx, symptom in enumerate(df_long['Symptom'].unique()):\n",
    "        ax = axes[col_idx, row_idx] if axes.ndim == 2 else axes[row_idx]\n",
    "        symptom_data = df_long[df_long['Symptom'] == symptom]\n",
    "        ytick_count = len(symptom_data['Number of samples'].unique())\n",
    "        y_positions = np.arange(ytick_count) * ytick_spacing\n",
    "\n",
    "        # Loop over subcategories\n",
    "        for k, subcat in enumerate(subcats):\n",
    "            for model in ['MedRoBERTa.nl', 'RobBERT']:\n",
    "                subset = symptom_data[\n",
    "                    (symptom_data['metric'] == subcat) &\n",
    "                    (symptom_data['Model'] == model)\n",
    "                ]\n",
    "                training_sizes = sorted(subset['Number of samples'].unique())\n",
    "                values = [\n",
    "                    subset[subset['Number of samples'] == ts]['mirrored_value'].values[0]\n",
    "                    if not subset[subset['Number of samples'] == ts].empty else np.nan\n",
    "                    for ts in training_sizes\n",
    "                ]\n",
    "                y_pos = y_positions + k * bar_width - bar_width \n",
    "\n",
    "                # Apply subcategory-specific color\n",
    "                if model == 'MedRoBERTa.nl':\n",
    "                    color = subcategory_colors_medroberta[metric].get(subcat, '#cccccc')\n",
    "                else:\n",
    "                    color = subcategory_colors_roberta[metric].get(subcat, '#cccccc')\n",
    "\n",
    "                ax.barh(\n",
    "                    y=y_pos,\n",
    "                    width=values,\n",
    "                    height=bar_width,\n",
    "                    color=color,\n",
    "                    edgecolor='none'\n",
    "                )\n",
    "\n",
    "        # Y-axis labels for the first row only\n",
    "        if row_idx == 0:\n",
    "            ax.set_yticks(y_positions + 0.15)\n",
    "            ax.set_yticklabels([1, 2, 3], fontsize=font_params['fontsize'])\n",
    "            ax.set_ylabel(\"Number of samples\", fontsize=font_params['fontsize'])\n",
    "        else:\n",
    "            ax.set_yticklabels([1, 2, 3])\n",
    "\n",
    "        ax.set_xlim(-1, 1)\n",
    "        ax.axvline(0, color='black', linewidth=0.5)\n",
    "        ax.grid(True, axis='x', linestyle='--', linewidth=0.5, color='black')\n",
    "\n",
    "        # Remove x-tick labels for non-last rows\n",
    "        if row_idx != num_symptoms - 1:\n",
    "            ax.set_xticklabels([])\n",
    "\n",
    "        # Set custom x-tick labels\n",
    "        xticks = np.linspace(-1, 1, 9)\n",
    "        xticklabels = ['1', '0.75', '0.5', '0.25', '0', '0.25', '0.50', '0.75', '1']\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xticklabels, fontsize=font_params['fontsize']-5)\n",
    "\n",
    "        ax.text(0.442, 1.005, 'MedRoBERTa.nl vs RobBERT', transform=ax.transAxes,\n",
    "                fontsize=font_params['fontsize']-1, ha='center', va='bottom')\n",
    "\n",
    "# Add \"Value\" label for each column in the last row\n",
    "for col_idx in range(num_symptoms):\n",
    "    ax = axes[len(metrics) - 1, col_idx]  \n",
    "    ax.set_xlabel(\"Value\", fontsize=font_params['fontsize']) \n",
    "\n",
    "# Build and add legend\n",
    "# Define unified submetric order and labels\n",
    "legend_submetrics = ['Macro', \"'present'\", \"'absent'\", \"'not reported'\"]\n",
    "label_mapping = {\n",
    "    'Macro': 'Macro',\n",
    "    \"'present'\": \"Present\",\n",
    "    \"'absent'\": \"Absent\",\n",
    "    \"'not reported'\": \"Not reported\"\n",
    "}\n",
    "\n",
    "# Use Recall mappings as representative for colors (they are consistent across metrics)\n",
    "color_keys_med = ['Macro-recall', \"Recall 'present'\", \"Recall 'absent'\", \"Recall 'not reported'\"]\n",
    "color_keys_rob = ['Macro-recall', \"Recall 'present'\", \"Recall 'absent'\", \"Recall 'not reported'\"]\n",
    "\n",
    "legend_items_medroberta = [TextArea(\"MedRoBERTa.nl\", textprops=dict(fontsize=font_params['fontsize'], ha='center'))]\n",
    "legend_items_roberta = [TextArea(\"RobBERT\", textprops=dict(fontsize=font_params['fontsize'], ha='center'))]\n",
    "\n",
    "# Add submetric entries just once\n",
    "for submetric_label, color_key_med, color_key_rob in zip(legend_submetrics, color_keys_med, color_keys_rob):\n",
    "    # MedRoBERTa.nl\n",
    "    patch_blue = DrawingArea(20, 10, 0, 0)\n",
    "    rect_blue = Rectangle((0, 0), 20, 10, fc=subcategory_colors_medroberta['Recall'][color_key_med], edgecolor='none')\n",
    "    patch_blue.add_artist(rect_blue)\n",
    "    label_blue = TextArea(label_mapping[submetric_label], textprops=dict(fontsize=font_params['fontsize'], ha='left'))\n",
    "    legend_items_medroberta.append(HPacker(children=[patch_blue, label_blue], align=\"left\", pad=0, sep=6))\n",
    "\n",
    "    # RobBERT\n",
    "    patch_orange = DrawingArea(20, 10, 0, 0)\n",
    "    rect_orange = Rectangle((0, 0), 20, 10, fc=subcategory_colors_roberta['Recall'][color_key_rob], edgecolor='none')\n",
    "    patch_orange.add_artist(rect_orange)\n",
    "    label_orange = TextArea(label_mapping[submetric_label], textprops=dict(fontsize=font_params['fontsize'], ha='left'))\n",
    "    legend_items_roberta.append(HPacker(children=[patch_orange, label_orange], align=\"left\", pad=0, sep=6))\n",
    "\n",
    "# Combine and place the legend\n",
    "final_legend = VPacker(\n",
    "    children=legend_items_medroberta + [TextArea(\"\", textprops=dict(fontsize=font_params['fontsize']))] + legend_items_roberta,\n",
    "    align=\"left\", pad=0, sep=18\n",
    ")\n",
    "\n",
    "anchored_box = AnchoredOffsetbox(\n",
    "    loc='center left',\n",
    "    child=final_legend,\n",
    "    pad=0.,\n",
    "    frameon=False,\n",
    "    bbox_to_anchor=(0.94, 0.5),\n",
    "    bbox_transform=fig.transFigure,\n",
    "    borderpad=0.\n",
    ")\n",
    "fig.add_artist(anchored_box)\n",
    "\n",
    "\n",
    "# Subtitle\n",
    "# Add column headers\n",
    "for idx, metric in enumerate(df_long['Symptom'].unique()):\n",
    "    fig.text(\n",
    "        x=(0.065 + idx * 0.102),  \n",
    "        y=0.98,\n",
    "        s=metric,\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=font_params['fontsize'] + 2,  \n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Add row labels (Recall, Precision, F1) from top to bottom\n",
    "row_labels = ['Recall', 'Precision', 'F1-score']\n",
    "for idx, label in enumerate(row_labels):\n",
    "    fig.text(\n",
    "        x=-0.01,  \n",
    "        y=0.81 - idx * 0.31, \n",
    "        s=label,\n",
    "        ha='left',\n",
    "        va='center',\n",
    "        rotation='vertical',\n",
    "        fontsize=font_params['fontsize'] + 2,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "fig.suptitle(\"Prompt-based classification\", fontsize=20, fontweight='bold', y=1.02, x=0.01, ha='left')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.93, 1])\n",
    "plt.savefig('Prompt-based_classifiers_mono.tiff', format='tiff',\n",
    "            bbox_inches='tight', \n",
    "            pad_inches=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9931a1-589c-4fb0-94ba-dafd4acf7dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
